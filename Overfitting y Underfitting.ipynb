{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f6de05",
   "metadata": {},
   "source": [
    "# Overfitting y Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f24e4a",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "- Comprender qu√© es el **overfitting** (sobreajuste) y el **underfitting** (subajuste)\n",
    "- Identificar visualmente estos fen√≥menos en curvas de entrenamiento\n",
    "- Aplicar t√©cnicas para prevenir overfitting en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc97cc8",
   "metadata": {},
   "source": [
    "## Importar librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_friedman3\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10889c4",
   "metadata": {},
   "source": [
    "## Cargar y preparar datos\n",
    "\n",
    "Usaremos el dataset de la central de ciclo combinado, pero **crearemos un escenario propenso al overfitting**:\n",
    "- Entrenaremos con solo el 30% de los datos (dataset peque√±o ‚Üí f√°cil overfitting)\n",
    "- Usaremos modelos complejos\n",
    "- Sin regularizaci√≥n inicialmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(n_samples, train_ratio, batch_size, seed=42):\n",
    "\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  \n",
    "  X, y =make_friedman3(n_samples=n_samples,\n",
    "                        noise=40,\n",
    "                        random_state=seed)\n",
    "  X_lb = np.array([0.,40.,0.,1.])\n",
    "  X_ub = np.array([100.,560*np.pi,1.,11.])\n",
    "  X_scaled = (X-X_lb)/(X_ub-X_lb)\n",
    "\n",
    "  dataset = {}\n",
    "  for i in range(X_scaled.shape[-1]):\n",
    "    dataset[f\"X_{i}\"] = X_scaled[:,i]\n",
    "  dataset[\"y\"] = y\n",
    "  dataset = pd.DataFrame(dataset)\n",
    "\n",
    "  X_train = dataset.sample(frac=train_ratio, random_state=42)\n",
    "  X_val = dataset.drop(X_train.index)\n",
    "\n",
    "  # Separar features y target\n",
    "  y_train = X_train.pop('y')\n",
    "  y_val = X_val.pop('y')\n",
    "\n",
    "  # Convertir a tensors de PyTorch\n",
    "  X_train_tensor = torch.FloatTensor(X_train.values.copy()).to(device)\n",
    "  y_train_tensor = torch.FloatTensor(y_train.values.copy()).reshape(-1, 1).to(device)\n",
    "\n",
    "  X_val_tensor = torch.FloatTensor(X_val.values.copy()).to(device)\n",
    "  y_val_tensor = torch.FloatTensor(y_val.values.copy()).reshape(-1, 1).to(device)\n",
    "\n",
    "  # Data loader\n",
    "  train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  print(f\"Train set: {X_train.shape[0]} muestras\")\n",
    "  print(f\"Val set: {X_val.shape[0]} muestras\")\n",
    "\n",
    "  return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.2\n",
    "BATCH_SIZE = 50\n",
    "NSAMPLES = 1000\n",
    "\n",
    "train_loader, val_loader = prepare_data(n_samples=NSAMPLES, \n",
    "                                        train_ratio=TRAIN_RATIO, \n",
    "                                        batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fcb6e",
   "metadata": {},
   "source": [
    "## Demostraci√≥n 1: Underfitting vs Overfitting (sin regularizaci√≥n)\n",
    "\n",
    "Entrenaremos dos modelos:\n",
    "1. **Modelo peque√±o** (16 ‚Üí 8 ‚Üí 1): Probablemente underfitting\n",
    "2. **Modelo grande** (128 ‚Üí 64 ‚Üí 32 ‚Üí 1): Probablemente overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    \"\"\"Modelo peque√±o para observar subajuste o underfitting\"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LargeModel(nn.Module):\n",
    "    \"\"\"Modelo grande para trabajar el sobreajuste o overfitting\"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Construir modelos\n",
    "input_size = train_loader.dataset.tensors[0].shape[1]\n",
    "device = train_loader.dataset.tensors[0].device\n",
    "small = SmallModel(input_size).to(device)\n",
    "large = LargeModel(input_size).to(device)\n",
    "\n",
    "# Conteo de par√°metros de la red neuronal\n",
    "print(f\"Par√°metros modelo peque√±o: {sum(p.numel() for p in small.parameters())}\")\n",
    "print(f\"Par√°metros modelo grande: {sum(p.numel() for p in large.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c34535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=200, lr=0.01, early_stopping=False, patience=20, min_delta=0.001):\n",
    "    \"\"\"\n",
    "    Funci√≥n para entrenar modelos.\n",
    "    \n",
    "    Par√°metros:\n",
    "        early_stopping: Si True, detiene cuando val_loss no mejora\n",
    "        patience: Cu√°ntas √©pocas esperar sin mejora\n",
    "        min_delta: m√≠nimo cambio en la funci√≥n objetivo que se considera mejora\n",
    "    \"\"\"\n",
    "    loss_fcn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training loop\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Training\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fcn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            with torch.no_grad():\n",
    "                y_pred_val = model(X_batch)\n",
    "                loss = loss_fcn(y_pred_val, y_batch)\n",
    "\n",
    "                val_loss += loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss.item() < best_val_loss - min_delta:\n",
    "                best_val_loss = val_loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping en √©poca {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        # if (epoch + 1) % 50 == 0:\n",
    "        #     print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 400\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo peque√±o (underfitting moderado)\n",
    "small = SmallModel(input_size).to(device)\n",
    "history_small = train_model(small, train_loader, val_loader, epochs=EPOCHS, lr=LR, early_stopping=False)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_small['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_small['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Entrenar modelo grande (overfitting)\n",
    "large = LargeModel(input_size).to(device)\n",
    "history_large = train_model(large, train_loader, val_loader, epochs=EPOCHS, lr=LR, early_stopping=False)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_large['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_large['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar underfitting vs overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Modelo peque√±o (underfitting)\n",
    "axes[0].plot(history_small['train_loss'], label='Train Loss', linewidth=2, color=\"c\")\n",
    "axes[0].plot(history_small['val_loss'], label='Val Loss', linewidth=2, color=\"r\")\n",
    "axes[0].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[0].set_title('Modelo Peque√±o (UNDERFITTING)\\nAmbas p√©rdidas altas y similares', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Modelo grande (overfitting)\n",
    "axes[1].plot(history_large['train_loss'], label='Train Loss', linewidth=2, color=\"c\")\n",
    "axes[1].plot(history_large['val_loss'], label='Val Loss', linewidth=2, color=\"r\")\n",
    "axes[1].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[1].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[1].set_title('Modelo Grande (OVERFITTING)\\nVal loss diverge de train loss', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä OBSERVACIONES:\")\n",
    "print(f\"Modelo peque√±o: brecha final = {history_small['val_loss'][-1] - history_small['train_loss'][-1]:.4f}\")\n",
    "print(f\"Modelo grande: brecha final = {history_large['val_loss'][-1] - history_large['train_loss'][-1]:.4f}\")\n",
    "print(f\"\\n‚û°Ô∏è El modelo grande tiene OVERFITTING (brecha > 0)\")\n",
    "print(f\"‚û°Ô∏è El modelo peque√±o tiene UNDERFITTING (ambas p√©rdidas altas)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
