{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828c647a",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning con Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8adedc",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "- Comprender qu√© son los **hyperpar√°metros** y por qu√© necesitan optimizaci√≥n\n",
    "- Aprender a usar **Optuna** para buscar autom√°ticamente la mejor configuraci√≥n\n",
    "- Definir espacios de b√∫squeda con diferentes tipos de par√°metros\n",
    "- Comparar configuraciones de modelos\n",
    "- Entrenar el modelo final con los mejores hyperpar√°metros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a11760",
   "metadata": {},
   "source": [
    "## Introducci√≥n a Hyperparameters\n",
    "\n",
    "### ¬øPar√°metros vs Hyperpar√°metros?\n",
    "\n",
    "**Par√°metros:**\n",
    "- Los **pesos** (weights) y **sesgos** (biases) de la red neuronal\n",
    "- Se aprenden autom√°ticamente durante el entrenamiento\n",
    "- NO se establecen manualmente\n",
    "\n",
    "**Hyperpar√°metros:**\n",
    "- Configuraciones que T√ö estableces antes de entrenar\n",
    "- NO se aprenden autom√°ticamente\n",
    "- Afectan C√ìMO el modelo aprende\n",
    "\n",
    "**Ejemplos de hyperpar√°metros:**\n",
    "- N√∫mero de capas (network depth)\n",
    "- N√∫mero de neuronas por capa (network width)\n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Tasa de Dropout\n",
    "- Coeficiente de regularizaci√≥n L2\n",
    "- Epochs, patience para early stopping\n",
    "\n",
    "### ¬øPor qu√© optimizar hyperpar√°metros?\n",
    "\n",
    "La elecci√≥n de hyperpar√°metros tiene un **GRAN IMPACTO** en el rendimiento final:\n",
    "- Learning rate muy alto ‚Üí Entrenamiento inestable\n",
    "- Learning rate muy bajo ‚Üí Convergencia lenta\n",
    "- Modelo muy peque√±o ‚Üí Underfitting\n",
    "- Modelo muy grande ‚Üí Overfitting\n",
    "- Batch size afecta estabilidad y velocidad\n",
    "\n",
    "**Soluci√≥n antigua:** B√∫squeda manual (lenta, subjetiva)\n",
    "**Soluci√≥n moderna:** B√∫squeda automatizada con **Optuna** ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1045600",
   "metadata": {},
   "source": [
    "## ¬øQu√© es Optuna?\n",
    "\n",
    "**Optuna** es un framework de c√≥digo abierto para optimizaci√≥n bayesiana de hiperpar√°metros.\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- üéØ Busca autom√°ticamente los mejores hyperpar√°metros\n",
    "- üìä Usa Machine Learning para guiar la b√∫squeda (Bayesian optimization)\n",
    "- ‚ö° Eficiente - prueba menos configuraciones que b√∫squeda exhaustiva\n",
    "- üêç Pyth√≥nico - API simple e intuitiva\n",
    "- üîß Flexible - funciona con cualquier framework (PyTorch, TensorFlow, sklearn, etc.)\n",
    "- üìà Visualizaciones √∫tiles del proceso de optimizaci√≥n\n",
    "\n",
    "**Instalaci√≥n:**\n",
    "```bash\n",
    "pip install optuna\n",
    "```\n",
    "\n",
    "**Documentaci√≥n:** [Optuna oficial](https://optuna.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911aae9",
   "metadata": {},
   "source": [
    "## Importar librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ada87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Configurar visualizaci√≥n\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Fijar seeds para reproducibilidad\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68723ed3",
   "metadata": {},
   "source": [
    "## Cargar y preparar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  data_path = 'https://raw.githubusercontent.com/cursos-COnCEPT/curso-tensorflow/refs/heads/main/CCP.csv'\n",
    "else:\n",
    "  data_path = os.getcwd() + '\\\\CCP.csv'\n",
    "\n",
    "dataset = pd.read_csv(data_path, sep=',')\n",
    "print(f\"Dataset shape: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n train-val-test (proporci√≥n est√°ndar)\n",
    "train_ratio = 0.70\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "X = dataset.sample(frac=train_ratio+val_ratio, random_state=42)\n",
    "X_test = dataset.drop(X.index)\n",
    "X_train = X.sample(frac=train_ratio/(val_ratio+train_ratio), random_state=42)\n",
    "X_val = X.drop(X_train.index)\n",
    "\n",
    "# Separar features y target\n",
    "y_train = X_train.pop('PE')\n",
    "y_test = X_test.pop('PE')\n",
    "y_val = X_val.pop('PE')\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizaci√≥n con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_val_norm = scaler.transform(X_val)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "# Convertir a tensors de PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_norm).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1).to(device)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val_norm).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).reshape(-1, 1).to(device)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test_norm).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1).to(device)\n",
    "\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(f\"Input size: {X_train_norm.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16513767",
   "metadata": {},
   "source": [
    "## Paso 1: Definir el espacio de b√∫squeda\n",
    "\n",
    "### ¬øQu√© par√°metros vamos a optimizar?\n",
    "\n",
    "Optaremos por optimizar los siguientes hyperpar√°metros:\n",
    "\n",
    "1. **`hidden_size1`:** N√∫mero de neuronas en la 1¬™ capa (16-256)\n",
    "2. **`hidden_size2`:** N√∫mero de neuronas en la 2¬™ capa (8-128)\n",
    "3. **`learning_rate`:** Opciones discretas [0.001, 0.005, 0.01]\n",
    "4. **`dropout_rate`:** Tasa de dropout (0.0-0.5)\n",
    "5. **`weight_decay`:** Regularizaci√≥n L2 (0.0-0.01)\n",
    "6. **`batch_size`:** Tama√±o de batch [32, 64, 128, 256]\n",
    "\n",
    "### Tipos de par√°metros en Optuna\n",
    "\n",
    "- **`trial.suggest_int()`:** Par√°metros enteros (ej: n√∫mero de neuronas)\n",
    "- **`trial.suggest_float()`:** Par√°metros continuos (ej: learning rate)\n",
    "- **`trial.suggest_categorical()`:** Opciones discretas (ej: batch size)\n",
    "\n",
    "**Referencia:** [`Optuna API - Trial object`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialModel(nn.Module):\n",
    "    \"\"\"Modelo parametrizado para Optuna\"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_size2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"Clase TrialModel definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Funci√≥n objetivo para Optuna.\n",
    "    Optuna intentar√° MINIMIZAR el valor retornado (validation loss).\n",
    "    \"\"\"\n",
    "    \n",
    "    # ============ DEFINIR ESPACIO DE B√öSQUEDA ============\n",
    "    # Par√°metros enteros para la arquitectura\n",
    "    hidden_size1 = trial.suggest_int('hidden_size1', 16, 256, step=16)\n",
    "    hidden_size2 = trial.suggest_int('hidden_size2', 8, 128, step=8)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    \n",
    "    # Par√°metros de entrenamiento\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [1e-3, 5e-3, 1e-2])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.01, log=True)  # log=True para espaciado logar√≠tmico\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    \n",
    "    # ============ CREAR MODELO ============\n",
    "    input_size = X_train_norm.shape[1]\n",
    "    model = TrialModel(input_size, hidden_size1, hidden_size2, dropout_rate).to(device)\n",
    "    \n",
    "    # ============ CONFIGURAR ENTRENAMIENTO ============\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # DataLoader con el batch_size sugerido\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # ============ ENTRENAMIENTO ============\n",
    "    epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(y_val_pred, y_val_tensor).item()\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss - 0.0001:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "        \n",
    "        # Informar a Optuna del progreso (permite pruning)\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        # Opcionalmente, Optuna puede detener ensayos prometedores\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "print(\"Funci√≥n objetivo definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314348ea",
   "metadata": {},
   "source": [
    "## Paso 2: Crear y ejecutar el Study de Optuna\n",
    "\n",
    "### ¬øQu√© es un Study?\n",
    "\n",
    "Un **Study** en Optuna es el objeto que orquesta toda la optimizaci√≥n:\n",
    "- Realiza m√∫ltiples \"trials\" (pruebas)\n",
    "- Guarda el historial de todos los trials\n",
    "- Usa Bayesian optimization para elegir qu√© probar despu√©s\n",
    "- Encuentra la mejor configuraci√≥n\n",
    "\n",
    "**Opciones principales:**\n",
    "- `direction='minimize'` o `'maximize'` - ¬øqu√© optimizamos?\n",
    "- `sampler` - Algoritmo de b√∫squeda (TPE por defecto - muy bueno)\n",
    "- `pruner` - Detiene trials malos temprano\n",
    "\n",
    "**Referencia:** [`optuna.create_study()`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un Study\n",
    "# TPE (Tree-structured Parzen Estimator) es el sampler por defecto y muy efectivo\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',  # Queremos minimizar validation loss\n",
    "    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "    pruner=optuna.pruners.MedianPruner(),  # Detiene trials que van mal\n",
    ")\n",
    "\n",
    "print(\"Study creado\")\n",
    "print(f\"Sampler: {study.sampler}\")\n",
    "print(f\"Pruner: {study.pruner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62025dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la optimizaci√≥n\n",
    "# Esto probar√° diferentes combinaciones de hyperpar√°metros\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INICIANDO OPTIMIZACI√ìN DE HYPERPAR√ÅMETROS CON OPTUNA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=20,  # N√∫mero de configuraciones a probar\n",
    "    show_progress_bar=True,\n",
    "    gc_after_trial=True,  # Liberar memoria despu√©s de cada trial\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZACI√ìN COMPLETADA\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10833694",
   "metadata": {},
   "source": [
    "## Paso 3: Analizar resultados de Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el mejor trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"\\nüìä MEJOR CONFIGURACI√ìN ENCONTRADA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Validation Loss: {best_trial.value:.6f}\")\n",
    "print(f\"\\nHyperpar√°metros:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75704a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de trials\n",
    "trials_df = study.trials_dataframe()\n",
    "print(f\"\\nResumen de {len(trials_df)} trials realizados:\")\n",
    "print(trials_df[['number', 'value', 'state']].head(10))\n",
    "\n",
    "# Estad√≠sticas\n",
    "print(f\"\\nEstad√≠sticas:\")\n",
    "print(f\"  Mejor valor (min): {trials_df['value'].min():.6f}\")\n",
    "print(f\"  Peor valor (max): {trials_df['value'].max():.6f}\")\n",
    "print(f\"  Promedio: {trials_df['value'].mean():.6f}\")\n",
    "print(f\"  Trials completados: {len(trials_df[trials_df['state'] == 'COMPLETE'])}\")\n",
    "print(f\"  Trials prunados: {len(trials_df[trials_df['state'] == 'PRUNED'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418185c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el historial de optimizaci√≥n\n",
    "fig = plot_optimization_history(study).show()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà El gr√°fico muestra c√≥mo el mejor valor encontrado mejora en cada trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310f37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancia de par√°metros\n",
    "fig = plot_param_importances(study).show()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Importancia de par√°metros:\")\n",
    "print(\"Muestra qu√© par√°metros tienen mayor impacto en el resultado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f994e",
   "metadata": {},
   "source": [
    "## Paso 4: Entrenar modelo final con los mejores hyperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed99070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los mejores hyperpar√°metros\n",
    "best_params = best_trial.params\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRENANDO MODELO FINAL CON MEJORES HYPERPAR√ÅMETROS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear el modelo\n",
    "input_size = X_train_norm.shape[1]\n",
    "final_model = TrialModel(\n",
    "    input_size,\n",
    "    hidden_size1=best_params['hidden_size1'],\n",
    "    hidden_size2=best_params['hidden_size2'],\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nModelo creado con arquitectura:\")\n",
    "print(f\"  Capa 1: {input_size} ‚Üí {best_params['hidden_size1']} + ReLU + Dropout({best_params['dropout_rate']:.3f})\")\n",
    "print(f\"  Capa 2: {best_params['hidden_size1']} ‚Üí {best_params['hidden_size2']} + ReLU + Dropout({best_params['dropout_rate']:.3f})\")\n",
    "print(f\"  Salida: {best_params['hidden_size2']} ‚Üí 1\")\n",
    "\n",
    "total_params = sum(p.numel() for p in final_model.parameters())\n",
    "print(f\"  Total de par√°metros: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba105ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar entrenamiento\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "    final_model.parameters(),\n",
    "    lr=best_params['learning_rate'],\n",
    "    weight_decay=best_params['weight_decay']\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "print(f\"\\nConfiguraci√≥n de entrenamiento:\")\n",
    "print(f\"  Learning rate: {best_params['learning_rate']}\")\n",
    "print(f\"  Weight decay (L2): {best_params['weight_decay']:.6f}\")\n",
    "print(f\"  Batch size: {best_params['batch_size']}\")\n",
    "print(f\"  Batches por √©poca: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo final\n",
    "epochs = 150\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\nEntrenando...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    final_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_mae = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        y_pred = final_model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        train_mae += torch.abs(y_pred - y_batch).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_mae /= len(train_loader.dataset)\n",
    "    \n",
    "    # Validation\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = final_model(X_val_tensor)\n",
    "        val_loss = criterion(y_val_pred, y_val_tensor).item()\n",
    "        val_mae = torch.abs(y_val_pred - y_val_tensor).mean().item()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_mae'].append(train_mae)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss - 0.0001:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping en √©poca {epoch}\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f\"√âpoca {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(f\"\\nEntrenamiento completado en {len(history['train_loss'])} √©pocas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb9f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el entrenamiento\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[0].set_title('Evoluci√≥n de la Funci√≥n de P√©rdida', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history['train_mae'], label='Train MAE', linewidth=2)\n",
    "axes[1].plot(history['val_mae'], label='Val MAE', linewidth=2)\n",
    "axes[1].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[1].set_ylabel('MAE (MW)', fontsize=12)\n",
    "axes[1].set_title('Evoluci√≥n del Error Absoluto Medio', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e61e55e",
   "metadata": {},
   "source": [
    "## Paso 5: Evaluaci√≥n final en test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en test set\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = final_model(X_test_tensor).cpu().numpy().flatten()\n",
    "    y_train_pred = final_model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Calcular m√©tricas\n",
    "y_test_np = y_test.values\n",
    "y_train_np = y_train.values\n",
    "\n",
    "test_mae = mean_absolute_error(y_test_np, y_test_pred)\n",
    "test_mse = mean_squared_error(y_test_np, y_test_pred)\n",
    "test_r2 = r2_score(y_test_np, y_test_pred)\n",
    "\n",
    "train_mae = mean_absolute_error(y_train_np, y_train_pred)\n",
    "train_mse = mean_squared_error(y_train_np, y_train_pred)\n",
    "train_r2 = r2_score(y_train_np, y_train_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUACI√ìN EN TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTrain:\")\n",
    "print(f\"  MAE: {train_mae:.4f} MW\")\n",
    "print(f\"  MSE: {train_mse:.4f}\")\n",
    "print(f\"  R¬≤: {train_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nTest:\")\n",
    "print(f\"  MAE: {test_mae:.4f} MW\")\n",
    "print(f\"  MSE: {test_mse:.4f}\")\n",
    "print(f\"  R¬≤: {test_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nGeneralizaci√≥n (Test - Train):\")\n",
    "print(f\"  MAE difference: {test_mae - train_mae:.4f} MW\")\n",
    "print(f\"  R¬≤ difference: {test_r2 - train_r2:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar predicciones\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Train set\n",
    "axes[0].scatter(y_train_np, y_train_pred, alpha=0.5, s=20)\n",
    "axes[0].plot([y_train_np.min(), y_train_np.max()], [y_train_np.min(), y_train_np.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Valor Real (MW)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicci√≥n (MW)', fontsize=12)\n",
    "axes[0].set_title(f'Train Set (MAE: {train_mae:.2f} MW)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axis('equal')\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test_np, y_test_pred, alpha=0.5, s=20, color='orange')\n",
    "axes[1].plot([y_test_np.min(), y_test_np.max()], [y_test_np.min(), y_test_np.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Valor Real (MW)', fontsize=12)\n",
    "axes[1].set_ylabel('Predicci√≥n (MW)', fontsize=12)\n",
    "axes[1].set_title(f'Test Set (MAE: {test_mae:.2f} MW)', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bd32f",
   "metadata": {},
   "source": [
    "## Resumen: Optuna vs B√∫squeda Manual\n",
    "\n",
    "### Ventajas de usar Optuna\n",
    "\n",
    "| Aspecto | B√∫squeda Manual | Optuna |\n",
    "|--------|------------------|--------|\n",
    "| **Tiempo** | ‚è±Ô∏è Horas/d√≠as | ‚ö° Minutos |\n",
    "| **Consistencia** | üòï Sesgos humanos | üéØ Reproducible |\n",
    "| **Eficiencia** | üìâ Prueba todo | üìà Aprende del historial |\n",
    "| **Documentaci√≥n** | üìù Dif√≠cil de seguir | üìä An√°lisis autom√°ticos |\n",
    "| **Escalabilidad** | üêå Dif√≠cil con muchos params | üöÄ Funciona con 10+ params |\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "- **Trial:** Una prueba con una combinaci√≥n de hyperpar√°metros\n",
    "- **Study:** El proceso completo de optimizaci√≥n\n",
    "- **Objective:** La funci√≥n que Optuna minimiza/maximiza\n",
    "- **Bayesian Optimization:** Algoritmo que aprende qu√© par√°metros funcionan mejor\n",
    "- **Pruning:** Detener trials que van mal temprano\n",
    "\n",
    "### Pr√≥ximos pasos\n",
    "\n",
    "1. **Expandir espacio de b√∫squeda:**\n",
    "   - Optimizar m√°s hyperpar√°metros\n",
    "   - Explorar diferentes arquitecturas (3+ capas)\n",
    "   - Incluir t√©cnicas de regularizaci√≥n\n",
    "\n",
    "2. **Usar callbacks y checkpoints:**\n",
    "   - Guardar el mejor modelo encontrado\n",
    "   - An√°lisis m√°s detallado de resultados\n",
    "\n",
    "3. **Cross-validation:**\n",
    "   - Evaluar en m√∫ltiples splits de datos\n",
    "   - Resultados m√°s robustos\n",
    "\n",
    "4. **Otras librer√≠as:**\n",
    "   - Ray Tune (para distribuido)\n",
    "   - Hyperopt\n",
    "   - Grid/Random search baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70ffac",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- [Optuna Documentaci√≥n Oficial](https://optuna.readthedocs.io/)\n",
    "- [PyTorch Optim](https://pytorch.org/docs/stable/optim.html)\n",
    "- [Hyperparameter Optimization - Andrew Ng](https://www.deeplearning.ai/)\n",
    "- [Practical Hyperparameter Optimization - Sebastian Raschka](https://sebastianraschka.com/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
