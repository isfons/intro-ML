{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f6de05",
   "metadata": {},
   "source": [
    "# Overfitting y Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f24e4a",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "- Comprender qu√© es el **overfitting** (sobreajuste) y el **underfitting** (subajuste)\n",
    "- Identificar visualmente estos fen√≥menos en curvas de entrenamiento\n",
    "- Aplicar t√©cnicas para prevenir overfitting en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc97cc8",
   "metadata": {},
   "source": [
    "## Importar librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_friedman3\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10889c4",
   "metadata": {},
   "source": [
    "## Cargar y preparar datos\n",
    "\n",
    "Usaremos el dataset de la central de ciclo combinado, pero **crearemos un escenario propenso al overfitting**:\n",
    "- Entrenaremos con solo el 30% de los datos (dataset peque√±o ‚Üí f√°cil overfitting)\n",
    "- Usaremos modelos complejos\n",
    "- Sin regularizaci√≥n inicialmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(n_samples, train_ratio, batch_size, seed=42):\n",
    "\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  \n",
    "  X, y =make_friedman3(n_samples=n_samples,\n",
    "                        noise=40,\n",
    "                        random_state=seed)\n",
    "  X_lb = np.array([0.,40.,0.,1.])\n",
    "  X_ub = np.array([100.,560*np.pi,1.,11.])\n",
    "  X_scaled = (X-X_lb)/(X_ub-X_lb)\n",
    "\n",
    "  dataset = {}\n",
    "  for i in range(X_scaled.shape[-1]):\n",
    "    dataset[f\"X_{i}\"] = X_scaled[:,i]\n",
    "  dataset[\"y\"] = y\n",
    "  dataset = pd.DataFrame(dataset)\n",
    "\n",
    "  X_train = dataset.sample(frac=train_ratio, random_state=42)\n",
    "  X_val = dataset.drop(X_train.index)\n",
    "\n",
    "  # Separar features y target\n",
    "  y_train = X_train.pop('y')\n",
    "  y_val = X_val.pop('y')\n",
    "\n",
    "  # Convertir a tensors de PyTorch\n",
    "  X_train_tensor = torch.FloatTensor(X_train.values.copy()).to(device)\n",
    "  y_train_tensor = torch.FloatTensor(y_train.values.copy()).reshape(-1, 1).to(device)\n",
    "\n",
    "  X_val_tensor = torch.FloatTensor(X_val.values.copy()).to(device)\n",
    "  y_val_tensor = torch.FloatTensor(y_val.values.copy()).reshape(-1, 1).to(device)\n",
    "\n",
    "  # Data loader\n",
    "  train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  print(f\"Train set: {X_train.shape[0]} muestras\")\n",
    "  print(f\"Val set: {X_val.shape[0]} muestras\")\n",
    "\n",
    "  return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.2\n",
    "BATCH_SIZE = 64\n",
    "NSAMPLES = 1000\n",
    "\n",
    "train_loader, val_loader = prepare_data(n_samples=NSAMPLES, \n",
    "                                        train_ratio=TRAIN_RATIO, \n",
    "                                        batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fcb6e",
   "metadata": {},
   "source": [
    "## Demostraci√≥n 1: Underfitting vs Overfitting (sin regularizaci√≥n)\n",
    "\n",
    "Entrenaremos dos modelos:\n",
    "1. **Modelo peque√±o** (16 ‚Üí 8 ‚Üí 1): Probablemente underfitting\n",
    "2. **Modelo grande** (128 ‚Üí 64 ‚Üí 32 ‚Üí 1): Probablemente overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    \"\"\"Modelo peque√±o para observar subajuste o underfitting\"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LargeModel(nn.Module):\n",
    "    \"\"\"Modelo grande para trabajar el sobreajuste o overfitting\"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Construir modelos\n",
    "input_size = train_loader.dataset.tensors[0].shape[1]\n",
    "device = train_loader.dataset.tensors[0].device\n",
    "small = SmallModel(input_size).to(device)\n",
    "large = LargeModel(input_size).to(device)\n",
    "\n",
    "# Conteo de par√°metros de la red neuronal\n",
    "print(f\"Par√°metros modelo peque√±o: {sum(p.numel() for p in small.parameters())}\")\n",
    "print(f\"Par√°metros modelo grande: {sum(p.numel() for p in large.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c34535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=200, lr=0.01, early_stopping=False, patience=20, min_delta=0.001):\n",
    "    \"\"\"\n",
    "    Funci√≥n para entrenar modelos.\n",
    "    \n",
    "    Par√°metros:\n",
    "        early_stopping: Si True, detiene cuando val_loss no mejora\n",
    "        patience: Cu√°ntas √©pocas esperar sin mejora\n",
    "        min_delta: m√≠nimo cambio en la funci√≥n objetivo que se considera mejora\n",
    "    \"\"\"\n",
    "    loss_fcn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training loop\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Training\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fcn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            with torch.no_grad():\n",
    "                y_pred_val = model(X_batch)\n",
    "                loss = loss_fcn(y_pred_val, y_batch)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        history['train_loss'].append(train_loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss.item() < best_val_loss - min_delta:\n",
    "                best_val_loss = val_loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping en √©poca {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        # if (epoch + 1) % 50 == 0:\n",
    "        #     print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 400\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo peque√±o (underfitting moderado)\n",
    "small = SmallModel(input_size).to(device)\n",
    "history_small = train_model(small, train_loader, val_loader, epochs=EPOCHS, lr=LR, early_stopping=False)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_small['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_small['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Entrenar modelo grande (overfitting)\n",
    "large = LargeModel(input_size).to(device)\n",
    "history_large = train_model(large, train_loader, val_loader, epochs=EPOCHS, lr=LR, early_stopping=False)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_large['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_large['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar underfitting vs overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Modelo peque√±o (underfitting)\n",
    "axes[0].plot(history_small['train_loss'], label='Train Loss', linewidth=2, color=\"c\")\n",
    "axes[0].plot(history_small['val_loss'], label='Val Loss', linewidth=2, color=\"r\")\n",
    "axes[0].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[0].set_title('Modelo Peque√±o (UNDERFITTING)\\nAmbas p√©rdidas altas y similares', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Modelo grande (overfitting)\n",
    "axes[1].plot(history_large['train_loss'], label='Train Loss', linewidth=2, color=\"c\")\n",
    "axes[1].plot(history_large['val_loss'], label='Val Loss', linewidth=2, color=\"r\")\n",
    "axes[1].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[1].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[1].set_title('Modelo Grande (OVERFITTING)\\nVal loss diverge de train loss', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä OBSERVACIONES:\")\n",
    "print(f\"Modelo peque√±o: brecha final = {history_small['val_loss'][-1] - history_small['train_loss'][-1]:.4f}\")\n",
    "print(f\"Modelo grande: brecha final = {history_large['val_loss'][-1] - history_large['train_loss'][-1]:.4f}\")\n",
    "print(f\"\\n‚û°Ô∏è El modelo grande tiene OVERFITTING (brecha > 0)\")\n",
    "print(f\"‚û°Ô∏è El modelo peque√±o tiene UNDERFITTING (ambas p√©rdidas altas)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar predicciones: Real vs Predicho (parity plot)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "small.eval()\n",
    "large.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_small = small(X_train).cpu().numpy().flatten()\n",
    "    y_pred_large = large(X_train).cpu().numpy().flatten()\n",
    "\n",
    "y_train_np = y_train.cpu().numpy().flatten()\n",
    "lims = [min(y_train_np.min(), y_pred_large.min(), y_pred_small.min()),\n",
    "        max(y_train_np.max(), y_pred_large.max(), y_pred_small.max())]\n",
    "\n",
    "# Modelo peque√±o (underfitting)\n",
    "axes[0].scatter(y_train_np, y_pred_small, alpha=0.6, s=50, color='blue', label='Predicciones')\n",
    "axes[0].plot(lims, lims, 'k--', linewidth=2, label='y = x')\n",
    "axes[0].set_xlabel('y real', fontsize=12)\n",
    "axes[0].set_ylabel('y predicho', fontsize=12)\n",
    "axes[0].set_title('Modelo Peque√±o (UNDERFITTING)', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Modelo grande (overfitting)\n",
    "axes[1].scatter(y_train_np, y_pred_large, alpha=0.6, s=50, color='red', label='Predicciones')\n",
    "axes[1].plot(lims, lims, 'k--', linewidth=2, label='y = x')\n",
    "axes[1].set_xlabel('y real', fontsize=12)\n",
    "axes[1].set_ylabel('y predicho', fontsize=12)\n",
    "axes[1].set_title('Modelo Grande (OVERFITTING)', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä INTERPRETACI√ìN:\")\n",
    "print(\"- Puntos cercanos a la diagonal indican buenas predicciones.\")\n",
    "print(\"- Dispersi√≥n alta indica underfitting o overfitting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3621c",
   "metadata": {},
   "source": [
    "## T√©cnica 1: Early Stopping\n",
    "\n",
    "### ¬øQu√© es Early Stopping?\n",
    "\n",
    "**Early Stopping** detiene autom√°ticamente el entrenamiento cuando la validaci√≥n loss deja de mejorar.\n",
    "\n",
    "**Ventajas:**\n",
    "- ‚úÖ Simple de implementar\n",
    "- ‚úÖ Muy efectivo para prevenir overfitting\n",
    "- ‚úÖ Ahorra tiempo de entrenamiento\n",
    "- ‚úÖ No a√±ade complejidad computacional\n",
    "\n",
    "**Par√°metros clave:**\n",
    "- `patience`: Cu√°ntas √©pocas esperar sin mejora antes de parar\n",
    "- `min_delta`: Cambio m√≠nimo para contar como \"mejora\"\n",
    "- `restore_best_weights`: Restaurar los mejores pesos (no los finales)\n",
    "\n",
    "**Referencia:** [`torch.optim` - PyTorch Optimizers](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced6591",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Entrenando modelo GRANDE con Early Stopping ===\")\n",
    "large_es = LargeModel(input_size).to(DEVICE)\n",
    "history_large_es = train_model(large_es, X_train, y_train, X_val, y_val, \n",
    "                               epochs=EPOCHS, lr=LR, early_stopping=True, patience=20)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_large_es['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_large_es['val_loss'][-1]:.4f}\")\n",
    "print(f\"√âpocas de entrenamiento: {len(history_large_es['train_loss'])} (de {EPOCHS})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar con y sin Early Stopping\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sin Early Stopping\n",
    "axes[0].plot(history_large['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history_large['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[0].set_title('SIN Early Stopping\\nVal loss contin√∫a aumentando', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Con Early Stopping\n",
    "axes[1].plot(history_large_es['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history_large_es['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[1].axvline(len(history_large_es['val_loss'])-1, color='red', linestyle='--', linewidth=2, label='Parada')\n",
    "axes[1].set_xlabel('√âpoca', fontsize=12)\n",
    "axes[1].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[1].set_title('CON Early Stopping\\nSe detiene cuando val_loss no mejora', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Early Stopping evita que el modelo contin√∫e empeorando en validaci√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2fd9a",
   "metadata": {},
   "source": [
    "## T√©cnica 2: Dropout\n",
    "\n",
    "### ¬øQu√© es Dropout?\n",
    "\n",
    "**Dropout** desactiva aleatoriamente una fracci√≥n de neuronas durante el entrenamiento.\n",
    "\n",
    "**Intuici√≥n:**\n",
    "- Previene que el modelo se vuelva codependiente de ciertas neuronas\n",
    "- For√ßa al modelo a aprender caracter√≠sticas redundantes y robustas\n",
    "- Es como entrenar m√∫ltiples modelos d√©biles en paralelo\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- Solo se aplica durante entrenamiento (`.train()`)\n",
    "- Durante evaluaci√≥n, todas las neuronas est√°n activas (`.eval()`)\n",
    "- T√≠picamente 20-50% de tasa de dropout\n",
    "\n",
    "**Referencia:** [`torch.nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0976dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeModelWithDropout(nn.Module):\n",
    "    \"\"\"Modelo grande CON Dropout para prevenir overfitting\"\"\"\n",
    "    def __init__(self, input_size, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "print(\"\\n=== Entrenando modelo GRANDE con Dropout (p=0.3) ===\")\n",
    "large_dropout = LargeModelWithDropout(input_size, dropout_rate=0.3).to(DEVICE)\n",
    "history_large_dropout = train_model(large_dropout, X_train, y_train, X_val, y_val, \n",
    "                                    epochs=EPOCHS, lr=LR, early_stopping=False)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_large_dropout['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_large_dropout['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532da16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar: Sin Dropout vs Con Dropout\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(history_large['train_loss'], label='Modelo Grande - Train', linewidth=2, linestyle='-')\n",
    "ax.plot(history_large['val_loss'], label='Modelo Grande - Val (OVERFITTING)', linewidth=2, linestyle='-')\n",
    "\n",
    "ax.plot(history_large_dropout['train_loss'], label='Con Dropout - Train', linewidth=2, linestyle='--', alpha=0.8)\n",
    "ax.plot(history_large_dropout['val_loss'], label='Con Dropout - Val', linewidth=2, linestyle='--', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('√âpoca', fontsize=12)\n",
    "ax.set_ylabel('MSE Loss', fontsize=12)\n",
    "ax.set_title('Efecto del Dropout en Overfitting', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Dropout reduce la brecha entre train y validation loss\")\n",
    "print(f\"Sin Dropout - brecha final: {history_large['val_loss'][-1] - history_large['train_loss'][-1]:.4f}\")\n",
    "print(f\"Con Dropout - brecha final: {history_large_dropout['val_loss'][-1] - history_large_dropout['train_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c08e8",
   "metadata": {},
   "source": [
    "## T√©cnica 3: Regularizaci√≥n L1/L2\n",
    "\n",
    "### ¬øQu√© es Regularizaci√≥n?\n",
    "\n",
    "**Regularizaci√≥n** a√±ade un t√©rmino de penalizaci√≥n a la funci√≥n de p√©rdida basado en el tama√±o de los pesos.\n",
    "\n",
    "**L2 (Ridge) Regularization:**\n",
    "$$\\text{Loss}_{total} = \\text{Loss}_{original} + \\lambda \\sum_i w_i^2$$\n",
    "\n",
    "- Penaliza pesos grandes\n",
    "- Favorece pesos peque√±os pero no necesariamente cero\n",
    "- Muy com√∫n en deep learning\n",
    "\n",
    "**L1 (Lasso) Regularization:**\n",
    "$$\\text{Loss}_{total} = \\text{Loss}_{original} + \\lambda \\sum_i |w_i|$$\n",
    "\n",
    "- Tiende a hacer algunos pesos exactamente cero\n",
    "- √ötil para selecci√≥n de caracter√≠sticas\n",
    "\n",
    "**En PyTorch:**\n",
    "- `weight_decay` en el optimizador implementa L2 regularization\n",
    "- Se aplica autom√°ticamente a todos los par√°metros entrenables\n",
    "\n",
    "**Referencia:** [`torch.optim.Adam` - weight_decay parameter](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88780df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_regularization(model, X_train, y_train, X_val, y_val, epochs=200, lr=0.01, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    Funci√≥n de entrenamiento con par√°metro weight_decay para L2 regularization.\n",
    "    \n",
    "    weight_decay: Coeficiente de regularizaci√≥n L2 (t√≠picamente 0.0001 - 0.01)\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training loop\"):\n",
    "        # Training\n",
    "        model.train()\n",
    "        y_pred_train = model(X_train)\n",
    "        train_loss = criterion(y_pred_train, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_val = model(X_val)\n",
    "            val_loss = criterion(y_pred_val, y_val)\n",
    "            \n",
    "            train_mae = torch.abs(y_pred_train - y_train).mean().item()\n",
    "            val_mae = torch.abs(y_pred_val - y_val).mean().item()\n",
    "        \n",
    "        history['train_loss'].append(train_loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['val_mae'].append(val_mae)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Entrenar con diferentes valores de L2 regularization\n",
    "print(\"\\n=== Modelo GRANDE con L2 Regularization (weight_decay=0.001) ===\")\n",
    "large_l2 = LargeModel(input_size).to(DEVICE)\n",
    "history_large_l2 = train_model_with_regularization(large_l2, X_train, y_train, X_val, y_val, \n",
    "                                                    epochs=EPOCHS, lr=LR, weight_decay=0.001)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_large_l2['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_large_l2['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c9997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar diferentes fuerzas de regularizaci√≥n\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(history_large['val_loss'], label='Sin Regularizaci√≥n', linewidth=2.5, marker='o', markersize=3, alpha=0.7)\n",
    "ax.plot(history_large_l2['val_loss'], label='L2 Regularization (Œª=0.001)', linewidth=2.5, marker='s', markersize=3, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('√âpoca', fontsize=12)\n",
    "ax.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax.set_title('Efecto de L2 Regularization en Validation Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ L2 Regularization suaviza las curvas y reduce overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb16562",
   "metadata": {},
   "source": [
    "## Comparaci√≥n: Todas las t√©cnicas\n",
    "\n",
    "Resumamos el rendimiento de todas las t√©cnicas en el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ce01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar todos los modelos en test set\n",
    "models_to_eval = {\n",
    "    'Peque√±o (Underfitting)': (small, 'orange'),\n",
    "    'Grande (Overfitting)': (large, 'red'),\n",
    "    'Grande + Early Stopping': (large_es, 'green'),\n",
    "    'Grande + Dropout': (large_dropout, 'blue'),\n",
    "    'Grande + L2': (large_l2, 'purple'),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, (model, color) in models_to_eval.items():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = model(X_test).cpu().numpy().flatten()\n",
    "    \n",
    "    y_test_np = y_test.values\n",
    "    mae = mean_absolute_error(y_test_np, y_pred_test)\n",
    "    mse = mean_squared_error(y_test_np, y_pred_test)\n",
    "    r2 = r2_score(y_test_np, y_pred_test)\n",
    "    \n",
    "    results[name] = {'MAE': mae, 'MSE': mse, 'R2': r2}\n",
    "\n",
    "# Crear tabla de resultados\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RENDIMIENTO EN TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string())\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a074e86",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "### Resumen de t√©cnicas para prevenir Overfitting\n",
    "\n",
    "| T√©cnica | Ventajas | Desventajas | Cu√°ndo usar |\n",
    "|---------|----------|------------|-------------|\n",
    "| **Early Stopping** | Simple, efectivo, r√°pido | Requiere validaci√≥n set | Casi siempre ‚úÖ |\n",
    "| **Dropout** | Muy efectivo, robusto | Aumenta tiempo entrenamiento | Modelos profundos/complejos |\n",
    "| **L1/L2 Regularizaci√≥n** | Suave, controlable | Hyperpar√°metro a tunar | Modelos simples/medianos |\n",
    "| **Reducir complejidad** | Soluciona el problema ra√≠z | Puede causar underfitting | Si el modelo es muy grande |\n",
    "| **M√°s datos** | Soluci√≥n ideal | Costoso/dif√≠cil de obtener | Si es posible |\n",
    "\n",
    "### Recomendaciones pr√°cticas\n",
    "\n",
    "1. **Siempre usar Early Stopping** - Es pr√°cticamente gratis y muy efectivo\n",
    "2. **Monitorear train vs val loss** - La brecha indica overfitting\n",
    "3. **Combinar t√©cnicas** - Early Stopping + Dropout + L2 es lo m√°s robusto\n",
    "4. **Tunar hyperpar√°metros** - `patience`, `dropout_rate`, `weight_decay`\n",
    "5. **Usar validaci√≥n set** - Cr√≠tico para detectar overfitting temprano\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "- **Underfitting:** Modelo demasiado simple ‚Üí Soluci√≥n: m√°s complejidad\n",
    "- **Overfitting:** Modelo memoriza ruido ‚Üí Soluci√≥n: regularizaci√≥n\n",
    "- **Trade-off:** Balance entre bias y varianza\n",
    "- **Generalizaci√≥n:** El verdadero objetivo del machine learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
