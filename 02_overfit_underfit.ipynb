{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f6de05",
   "metadata": {},
   "source": [
    "# 02 - Overfitting y Underfitting\n",
    "\n",
    "## **Objetivos**\n",
    "\n",
    "- Comprender qu√© es el **overfitting** (sobreajuste) y el **underfitting** (subajuste)\n",
    "- Identificar visualmente estos fen√≥menos en curvas de entrenamiento\n",
    "- Aplicar t√©cnicas para prevenir overfitting en Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc97cc8",
   "metadata": {},
   "source": [
    "## **Importar librer√≠as**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Move to top content folder\n",
    "    while not os.getcwd().endswith(\"content\"):\n",
    "        os.chdir(\"..\")\n",
    "    # Check if repo has already been cloned\n",
    "    if not os.path.exists(\"intro-ML\"): # if not, clone it\n",
    "      print(\"Cloning repo...\")\n",
    "      !git clone https://github.com/isfons/intro-ML.git\n",
    "    # Set the correct working directory\n",
    "    %cd intro-ML\n",
    "    # Update repo\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a920aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "torch.manual_seed(42)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fcb6e",
   "metadata": {},
   "source": [
    "## **Demostraci√≥n Underfitting vs Overfitting**\n",
    "\n",
    "### Definici√≥n del modelo\n",
    "\n",
    "La capacidad de un modelo se refiere al tama√±o y la complejidad de los patrones que es capaz de aprender. En el caso de las redes neuronales, depende del n√∫mero de neuronas que tenga y de c√≥mo est√©n conectadas entre s√≠. Por tanto, si parece que una red no se ajusta bien a los datos, se debe aumentar su capacidad.\n",
    "\n",
    "Hay dos maneras de aumentar la capacidad de una red neuronal:\n",
    "- Red m√°s ancha (a√±adiendo m√°s neuronas a las capas existentes)  &rarr;  m√°s f√°cil aprender relaciones  lineales\n",
    "- Red m√°s profunda (a√±adiendo m√°s capas)  &rarr; mejores en captar relaciones no lineales\n",
    "\n",
    "Para aproximar la funci√≥n peaks, utilizaremos dos modelos de redes neuronales:\n",
    "1. Modelo peque√±o (8 ‚Üí 1)\n",
    "2. Modelo grande (128 ‚Üí 64 ‚Üí 32 ‚Üí 1)\n",
    "\n",
    "üìù **Tarea:** completa el c√≥digo para definir la arquitectura del modelo grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallModel(nn.Module):\n",
    "    \"\"\"Modelo peque√±o para observar subajuste o underfitting\"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.net = ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return ...\n",
    "\n",
    "class LargeModel(nn.Module):\n",
    "    \"\"\"Modelo grande para trabajar el sobreajuste o overfitting\"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c60abe",
   "metadata": {},
   "source": [
    "### M√©todo de entrenamiento com√∫n\n",
    "- Funci√≥n de p√©rdidas: error cuadr√°tico medio o **MSE** (regresi√≥n)\n",
    "- Algoritmo de optimizaci√≥n: **Adam**\n",
    "- Learning rate: **valor fijo** durante todo el proceso de entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=200, lr=0.01):\n",
    "    \"\"\"\n",
    "    Funci√≥n para entrenar modelos.\n",
    "    \"\"\"\n",
    "    loss_fcn = ...\n",
    "    optimizer = ...\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training loop\"):\n",
    "        # Training\n",
    "        \n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd31b83",
   "metadata": {},
   "source": [
    "### Datos\n",
    "\n",
    "La **funci√≥n peaks** es una superficie tridimensional compleja que parece tener varios \"picos\" y \"valles\". Es como un paisaje monta√±oso con m√∫ltiples cimas.\n",
    "<center>\n",
    "<pre><code class=\"language-python\">\n",
    "y = peaks(x1, x2)\n",
    "</code></pre>\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Sergiy-Reutskiy/publication/257397025/figure/fig2/AS:798646559862785@1567423586780/The-PEAKS-function-F2x.png\" width=\"400\">\n",
    "</center>\n",
    "\n",
    "Para demostrar el overfitting (sobreajuste), necesitamos:\n",
    "1. **Datos insuficientes**: Solo usaremos 200 muestras, que es poco para aprender un patr√≥n tan complejo\n",
    "2. **Ruido**: A√±adimos valores aleatorios a los datos, simulando imprecisiones del mundo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65716745",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE = 2.\n",
    "TRAIN_RATIO = 0.3\n",
    "N_SAMPLES = 200\n",
    "BATCH_SIZE = int(TRAIN_RATIO*N_SAMPLES/2)\n",
    "\n",
    "peaks = PeaksFunction()\n",
    "train_loader, val_loader = peaks.prepare_dataset(n_samples=N_SAMPLES, test_size=(1-TRAIN_RATIO), batch_size=BATCH_SIZE, noise=NOISE)\n",
    "peaks.plot_scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b4930",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "Observa lo que ocurre al entrenar ambos modelos durante 500 √©pocas y con un *learning rate* de 0.005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "LR = 0.005\n",
    "\n",
    "# Entrenar modelo peque√±o (underfitting moderado)\n",
    "input_size = train_loader.dataset.tensors[0].shape[1]\n",
    "device = train_loader.dataset.tensors[0].device\n",
    "small = SmallModel(input_size).to(device)\n",
    "history_small = train_model(small, train_loader, val_loader, epochs=EPOCHS, lr=LR)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_small['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_small['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Entrenar modelo grande (overfitting)\n",
    "large = LargeModel(input_size).to(device)\n",
    "history_large = train_model(large, train_loader, val_loader, epochs=EPOCHS, lr=LR)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_large['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_large['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Representar curvas de aprendizaje\n",
    "plot_learning_curve_comparison(history_small, history_large)\n",
    "\n",
    "# Visualizar predicciones de ambos modelos\n",
    "fig_small = peaks.plot_predictions_surface(small, device, n_points=25, title='Modelo Peque√±o (Underfitting)')\n",
    "fig_large = peaks.plot_predictions_surface(large, device, n_points=25, title='Modelo Grande (Overfitting)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e06f51",
   "metadata": {},
   "source": [
    "## **Estrategias para prevenir el overfitting**\n",
    "\n",
    "### **1 - Early Stopping**\n",
    "\n",
    "**Early Stopping** detiene autom√°ticamente el entrenamiento cuando la validaci√≥n loss deja de mejorar.\n",
    "\n",
    "**Ventajas:**\n",
    "- Simple de implementar\n",
    "- Muy efectivo para prevenir overfitting\n",
    "- Ahorra tiempo de entrenamiento\n",
    "\n",
    "**Par√°metros clave:**\n",
    "- `patience`: Cu√°ntas √©pocas esperar sin mejora antes de parar\n",
    "- `min_delta`: Cambio m√≠nimo para contar como \"mejora\"\n",
    "\n",
    "üìù **Tarea**: adapta el m√©todo de entrenamiento para incluir early stopping de una forma sencilla. En la pr√°ctica, este m√©todo se aplica utilizando programaci√≥n orientada a objetos. Para conocer m√°s, consulta este [art√≠culo](https://medium.com/biased-algorithms/a-practical-guide-to-implementing-early-stopping-in-pytorch-for-model-training-99a7cbd46e9d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c34535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=200, lr=0.01, early_stopping=False, patience=20, min_delta=0.001):\n",
    "    \"\"\"\n",
    "    Funci√≥n para entrenar modelos.\n",
    "    \n",
    "    Par√°metros:\n",
    "        early_stopping: Si True, detiene cuando val_loss no mejora\n",
    "        patience: Cu√°ntas √©pocas esperar sin mejora\n",
    "        min_delta: m√≠nimo cambio en la funci√≥n objetivo que se considera mejora\n",
    "    \"\"\"\n",
    "    loss_fcn = ...\n",
    "    optimizer = ...\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    # Inicializa las variables para monitorear el error en validaci√≥n\n",
    "    best_val_loss = float(1e15)\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training loop\"):\n",
    "        # Training\n",
    "        \n",
    "\n",
    "        # Validation\n",
    "        \n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            improvement = ...\n",
    "            if improvement > min_delta:\n",
    "                \n",
    "\n",
    "            else:\n",
    "                \n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "LR = 0.005\n",
    "PATIENCE = 50\n",
    "MIN_DELTA = 1e-4\n",
    "EARLY_STOP = True\n",
    "\n",
    "# Entrenar modelo grande (overfitting)\n",
    "large_earlystop = LargeModel(input_size).to(device)\n",
    "history_earlystop = train_model(large_earlystop, train_loader, val_loader, epochs=EPOCHS, lr=LR, early_stopping = EARLY_STOP, patience=PATIENCE, min_delta = MIN_DELTA)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_earlystop['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_earlystop['val_loss'][-1]:.4f}\")\n",
    "\n",
    "plot_learning_curve(history_earlystop)\n",
    "fig_earlystop = peaks.plot_predictions_surface(large_earlystop, device, n_points=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab2846",
   "metadata": {},
   "source": [
    "### **2 - Dropout**\n",
    "\n",
    "**Dropout** desactiva aleatoriamente una fracci√≥n de neuronas durante el entrenamiento, siendo una de las t√©cnicas de regularizaci√≥n de redes neuronales m√°s eficaces y utilizadas. Por ejemplo, si la salida de una capa oculta fuera un vector `[0,2, 0,5, 1,3, 0,8, 1,1]`, despu√©s de aplicar el dropout, alguno de sus elementos tomar√°n el valor de 0 al azar `[0, 0,5, 1,3, 0, 1,1]`.\n",
    "\n",
    "La fracci√≥n de las caracter√≠sticas que se anulan se conoce como *dropout rate* ($p$).\n",
    "\n",
    "**Caracter√≠sticas:**\n",
    "- Solo se aplica durante entrenamiento (`.train()`)\n",
    "- Durante evaluaci√≥n, todas las neuronas est√°n activas (`.eval()`)\n",
    "- Valores t√≠picos: $0.2 \\leq p \\leq 0.5$\n",
    "\n",
    "Para aplicarlo, se incluye como una \"capa adicional\" tipo [`torch.nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) despu√©s de la funci√≥n de activaci√≥n.\n",
    "\n",
    "üìù **Tarea**: bas√°ndote en el modelo `LargeModel`, realiza las modificaciones necesarias para a√±adir dropout en todas las capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeModelDropout(nn.Module):\n",
    "    \"\"\"Modelo grande CON Dropout para prevenir overfitting\"\"\"\n",
    "    def __init__(self, input_size, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.net = ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a16c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "LR = 0.005\n",
    "DROPOUT_RATE = 0.3\n",
    "EARLY_STOP = False\n",
    "\n",
    "# Entrenar modelo con dropout\n",
    "large_dropout = LargeModelDropout(input_size, dropout_rate = DROPOUT_RATE).to(device)\n",
    "history_dropout = train_model(large_dropout, train_loader, val_loader, epochs=EPOCHS, lr=LR, early_stopping = EARLY_STOP)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_dropout['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_dropout['val_loss'][-1]:.4f}\")\n",
    "\n",
    "plot_learning_curve(history_dropout)\n",
    "fig_dropout = peaks.plot_predictions_surface(large_dropout, device, n_points=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar: Sin Dropout vs Con Dropout\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(history_large['train_loss'], label='Sin Dropout - Train', color=\"red\")\n",
    "ax.plot(history_large['val_loss'], label='Sin Dropout - Val', color=\"blue\")\n",
    "\n",
    "ax.plot(history_dropout['train_loss'], label='Con Dropout - Train', color=\"red\",  alpha = 0.5)\n",
    "ax.plot(history_dropout['val_loss'], label='Con Dropout - Val', color=\"blue\", alpha = 0.5)\n",
    "\n",
    "ax.set_xlim(0,len(history_dropout[\"train_loss\"]))\n",
    "ax.set_xlabel('√âpoca', fontsize=12)\n",
    "ax.set_ylabel('MSE Loss', fontsize=12)\n",
    "ax.set_title('Efecto del Dropout en Overfitting', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6258d8",
   "metadata": {},
   "source": [
    "### **3 - Regularizaci√≥n L1/L2**\n",
    "\n",
    "Una forma com√∫n de mitigar el sobreajuste es forzando a que los pesos tomen valores peque√±os. La **regularizaci√≥n** a√±ade un t√©rmino de penalizaci√≥n a la funci√≥n de p√©rdida basado en el tama√±o de los pesos. El par√°metro $\\lambda$ se denomina *weight decay*.\n",
    "\n",
    "**L2 (Ridge) Regularization:**\n",
    "$$\\text{Loss}_{total} = \\text{Loss}_{original} + \\lambda \\sum_i w_i^2$$\n",
    "\n",
    "- Penaliza pesos grandes\n",
    "- Favorece pesos peque√±os pero no necesariamente cero\n",
    "- Muy com√∫n en deep learning, ya que evita el fen√≥meno de [*vanishing gradients*](https://www.youtube.com/watch?v=8z3DFk4VxRo)\n",
    "\n",
    "**L1 (Lasso) Regularization:**\n",
    "$$\\text{Loss}_{total} = \\text{Loss}_{original} + \\lambda \\sum_i |w_i|$$\n",
    "\n",
    "- Tiende a hacer algunos pesos exactamente cero\n",
    "- √ötil para selecci√≥n de caracter√≠sticas\n",
    "\n",
    "**En PyTorch:**\n",
    "- Opci√≥n manual: calcular los t√©rminos L1 y L2 dentro del training loop y sumarlos al valor de la funci√≥n de p√©rdidas\n",
    "- Opci√≥n autom√°tica: la mayor√≠a de los optimizadores incluyen la opci√≥n de especificar el valor de $\\lambda$, aplicando internamente la regularizaci√≥n L2.\n",
    "\n",
    "<center>\n",
    "<pre><code class=\"language-python\">\n",
    "optimizer = SGD(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "</code></pre>\n",
    "</center>\n",
    "\n",
    "üí° **Nota:** para el algoritmo [Adam](https://docs.pytorch.org/docs/stable/generated/torch.optim.adam.Adam_class.html#adam), `weight_decay`>0 no es exactamente regularizaci√≥n L2. En ese caso, es preferible utilizar [AdamW](https://docs.pytorch.org/docs/stable/generated/torch.optim.adamw.AdamW_class.html).\n",
    "\n",
    "üìù **Tarea:** completa el c√≥digo incluyendo c√≥mo calcular los t√©rminos L1 y L2. Despu√©s, compara con la opci√≥n ya implementada en Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=200, lr=0.01, early_stopping=False, patience=20, min_delta=0.001, regularization=None, weight_decay = 0.001):\n",
    "    \"\"\"\n",
    "    Funci√≥n para entrenar modelos.\n",
    "    \n",
    "    Par√°metros:\n",
    "        early_stopping: Si True, detiene cuando val_loss no mejora\n",
    "        patience: Cu√°ntas √©pocas esperar sin mejora\n",
    "        min_delta: m√≠nimo cambio en la funci√≥n objetivo que se considera mejora\n",
    "    \"\"\"\n",
    "    loss_fcn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    # Inicializa las variables para monitorear el error en validaci√≥n\n",
    "    best_val_loss = float(1e15)\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training loop\"):\n",
    "        # Training\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        \n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        \n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "LR = 0.005\n",
    "EARLY_STOP = False\n",
    "REG = \"L2\"\n",
    "LAMBDA = 0.001\n",
    "\n",
    "# Entrenar modelo con dropout\n",
    "large_reg = LargeModelDropout(input_size, dropout_rate = DROPOUT_RATE).to(device)\n",
    "history_reg = train_model(large_reg, train_loader, val_loader, epochs=EPOCHS, lr=LR, early_stopping = EARLY_STOP, regularization=REG, weight_decay=LAMBDA)\n",
    "\n",
    "print(f\"\\nTrain loss final: {history_reg['train_loss'][-1]:.4f}\")\n",
    "print(f\"Val loss final: {history_reg['val_loss'][-1]:.4f}\")\n",
    "\n",
    "plot_learning_curve(history_reg)\n",
    "fig_dropout = peaks.plot_predictions_surface(large_reg, device, n_points=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173634f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar: Sin Regularizaci√≥n vs Con Regularizaci√≥n\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(history_large['train_loss'], label='Sin Regularizaci√≥n - Train', color=\"red\")\n",
    "ax.plot(history_large['val_loss'], label='Sin Regularizaci√≥n - Val', color=\"blue\")\n",
    "\n",
    "ax.plot(history_reg['train_loss'], label=f'{REG} - Train', color=\"red\",  alpha = 0.5)\n",
    "ax.plot(history_reg['val_loss'], label=f'{REG} - Val', color=\"blue\", alpha = 0.5)\n",
    "\n",
    "ax.set_xlim(0,len(history_reg[\"train_loss\"]))\n",
    "ax.set_xlabel('√âpoca', fontsize=12)\n",
    "ax.set_ylabel('MSE Loss', fontsize=12)\n",
    "ax.set_title('Efecto del Dropout en Overfitting', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
