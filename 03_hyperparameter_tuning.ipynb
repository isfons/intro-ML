{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828c647a",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning con Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8adedc",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "- Comprender qué son los **hyperparámetros** y por qué necesitan optimización\n",
    "- Aprender a usar **Optuna** para buscar automáticamente la mejor configuración\n",
    "- Definir espacios de búsqueda con diferentes tipos de parámetros\n",
    "- Comparar configuraciones de modelos\n",
    "- Entrenar el modelo final con los mejores hyperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Move to top content folder\n",
    "    while not os.getcwd().endswith(\"content\"):\n",
    "        os.chdir(\"..\")\n",
    "    # Check if repo has already been cloned\n",
    "    if not os.path.exists(\"intro-ML\"): # if not, clone it\n",
    "      print(\"Cloning repo...\")\n",
    "      !git clone https://github.com/isfons/intro-ML.git\n",
    "    # Set the correct working directory\n",
    "    %cd intro-ML\n",
    "    # Update repo\n",
    "    !git pull\n",
    "\n",
    "if not shutil.which(\"optuna\"):\n",
    "    !pip install -q optuna\n",
    "assert(shutil.which(\"optuna\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from utils import plot_learning_curve, plot_pred_vs_obs\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Fijar seeds para reproducibilidad\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43701339",
   "metadata": {},
   "source": [
    "# **Cargar datos**\n",
    "\n",
    "En esta demostración se va a utilizar la base de datos de la central de ciclo combinado, disponible en [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3174579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Cargar datos de la central eléctrica ciclo combinado desde un archivo .csv\"\"\"\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "      dataset_path = 'https://raw.githubusercontent.com/cursos-COnCEPT/curso-tensorflow/refs/heads/main/CCP.csv'\n",
    "    else:\n",
    "      dataset_path = os.getcwd() + '\\\\CCP.csv'\n",
    "\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "    return dataset\n",
    "\n",
    "def split_data(dataset):\n",
    "    \"\"\"Separar datos en train/validation/test.\"\"\"\n",
    "    train_ratio = 0.70\n",
    "    val_ratio = 0.15\n",
    "\n",
    "    X = dataset.sample(frac=train_ratio+val_ratio, random_state=42)\n",
    "    X_test = dataset.drop(X.index)\n",
    "    X_train = X.sample(frac=train_ratio/(val_ratio+train_ratio), random_state=42)\n",
    "    X_val = X.drop(X_train.index)\n",
    "\n",
    "    y_train = X_train.pop('PE')\n",
    "    y_test = X_test.pop('PE')\n",
    "    y_val = X_val.pop('PE')\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def scale_inputs(X_train, X_val, X_test):\n",
    "    \"\"\"Aplicar normalización Z-scores a los datos de entrada.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm  = scaler.transform(X_test)\n",
    "    X_val_norm   = scaler.transform(X_val)\n",
    "\n",
    "    return X_train_norm, X_val_norm, X_test_norm\n",
    "\n",
    "def create_datasets(X_train, y_train, X_val, y_val, X_test, y_test, device):\n",
    "    \"\"\"Transformar NumPy arrays a TensorDataset.\"\"\"\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values.copy()).reshape(-1, 1).to(device)\n",
    "\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.values.copy()).reshape(-1, 1).to(device)\n",
    "\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.values.copy()).reshape(-1, 1).to(device)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_ds = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88974421",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ds = load_data()\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(ds)\n",
    "X_train_norm, X_val_norm, X_test_norm = scale_inputs(X_train, X_val, X_test)\n",
    "train_ds, val_ds, test_ds = create_datasets(X_train_norm, y_train, X_val_norm, y_val, X_test_norm, y_test, device)\n",
    "input_size = X_train.shape[-1]\n",
    "\n",
    "print(f\"El entrenamiento se va a ejecutar en {device}.\\nNúmero de variables de entrada a la red neuronal: {input_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0b238",
   "metadata": {},
   "source": [
    "# **Resumen:** pasos básicos para el entrenamiento de una red neuronal\n",
    "**Paso 1:** Definir el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de red neuronal con número configurable de capas ocultas y neuronas.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Dimensión de entrada\n",
    "        device: Dispositivo (CPU o CUDA)\n",
    "        hidden_units: Número de neuronas en cada capa oculta\n",
    "        hidden_layers: Número de capas ocultas\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, device, hidden_units = 10, hidden_layers = 1, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(nn.Linear(in_features, hidden_units))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = hidden_units\n",
    "        layers.append(nn.Linear(in_features, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877bbcd",
   "metadata": {},
   "source": [
    "**Paso 2:** Configurar el método de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ef536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs = 200, lr = 0.001, show_progress_bar = True):\n",
    "    \"\"\"Entrena una red neuronal y devuelve la evolución de la función de pérdidas.\"\"\"\n",
    "    loss_fcn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    device = train_loader.dataset.tensors[0].device\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs), disable = show_progress_bar):\n",
    "\n",
    "        model.train()  \n",
    "        train_loss = 0.0      \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            predictions = model(X_batch)\n",
    "            loss = loss_fcn(predictions, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()    \n",
    "            optimizer.step()       \n",
    "            \n",
    "            train_loss += loss.item() \n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()  \n",
    "        val_loss = 0.0        \n",
    "        with torch.no_grad():  \n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                predictions = model(X_batch)\n",
    "                loss = loss_fcn(predictions, y_batch)\n",
    "                \n",
    "                val_loss += loss.item() \n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb0102",
   "metadata": {},
   "source": [
    "**Paso 3:** Entrenar el modelo para una combinación fija de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hidden_units, hidden_layers, lr, batch_size, dropout_rate, epochs):\n",
    "    \"\"\"Ejecuta un experimento de entrenamiento con los hiperparámetros especificados.\"\"\"   \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    \n",
    "    model = FlexibleModel(input_size = X_train.shape[-1],\n",
    "                          device = device,\n",
    "                          hidden_layers=hidden_layers,\n",
    "                          hidden_units=hidden_units,\n",
    "                          dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    history = train_model(model, train_loader, val_loader, epochs=epochs, lr=lr, show_progress_bar = False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1,2, figsize=(14,5))\n",
    "    plot_learning_curve(history,axes[0])\n",
    "    plot_pred_vs_obs(model, X_test_norm, y_test, axes[1], label = \"Test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a11760",
   "metadata": {},
   "source": [
    "# **Introducción a los hiperparámetros**\n",
    "\n",
    "### ¿Parámetros vs Hyperparámetros?\n",
    "\n",
    "**Parámetros:**\n",
    "- Los **pesos** (weights) y **sesgos** (biases) de la red neuronal\n",
    "- Se aprenden automáticamente durante el entrenamiento\n",
    "- NO se establecen manualmente\n",
    "\n",
    "**Hyperparámetros:**\n",
    "- Configuraciones que deben fijarse **antes** de entrenar\n",
    "- NO se aprenden automáticamente\n",
    "\n",
    "**Ejemplos de hyperparámetros:**\n",
    "- Número de capas \n",
    "- Número de neuronas por capa \n",
    "- Learning rate\n",
    "- Batch size\n",
    "- Tasa de Dropout\n",
    "- Coeficiente de regularización L2\n",
    "- Epochs, patience para early stopping\n",
    "\n",
    "### ¿Por qué optimizar hyperparámetros?\n",
    "\n",
    "La elección de hyperparámetros tiene un **GRAN IMPACTO** en el rendimiento final:\n",
    "- Learning rate muy alto → Entrenamiento inestable\n",
    "- Learning rate muy bajo → Convergencia lenta\n",
    "- Modelo muy pequeño → Underfitting\n",
    "- Modelo muy grande → Overfitting\n",
    "- Batch size afecta estabilidad y velocidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15395bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interact(run_experiment,\n",
    "                 hidden_units = widgets.IntSlider(min=10, max=250, step=10, value=10, description=\"Hidden units\", continuous_update = False),\n",
    "                 hidden_layers = widgets.IntSlider(min=1, max=4, value=1, step=1, description=\"Hidden layers\", continuous_update = False),\n",
    "                 dropout_rate = widgets.FloatSlider(min=0.0, max=0.5, step=0.1, value=0.0, description=\"Dropout rate\", continuous_update=False),\n",
    "                 lr = widgets.FloatLogSlider(min=-5, max=-1, step = 1, value = -3, base=10, description=\"Learning rate\", continuous_update = False),\n",
    "                 batch_size = widgets.IntSlider(min=50, max=500, step=50, value=150, description=\"Batch size\", continuous_update = False),\n",
    "                 epochs = widgets.IntSlider(min=50, max=500, step = 25, value = 50, description = \"Epochs\", continuous_update = False)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1045600",
   "metadata": {},
   "source": [
    "# **Optuna**\n",
    "\n",
    "**Optuna** es una librería de Python para el ajuste de hiperparámetros utilizando principios de optimización bayesiana.\n",
    "\n",
    "**Conceptos clave:**\n",
    "\n",
    "- **Trial:** Una prueba con una combinación de hyperparámetros\n",
    "- **Study:** El proceso completo de optimización\n",
    "- **Objective:** La función que Optuna minimiza/maximiza\n",
    "- **Bayesian Optimization:** Algoritmo que aprende qué parámetros funcionan mejor\n",
    "- **Pruning:** Detener trials que van mal temprano\n",
    "\n",
    "\n",
    "**Instalación:**\n",
    "```bash\n",
    "pip install optuna\n",
    "```\n",
    "\n",
    "**Documentación:** [Optuna oficial](https://optuna.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16513767",
   "metadata": {},
   "source": [
    "## Paso 1: Definir el espacio de búsqueda\n",
    "\n",
    "### ¿Qué parámetros vamos a optimizar?\n",
    "\n",
    "Optaremos por optimizar los siguientes hyperparámetros:\n",
    "\n",
    "1. **`hidden_size1`:** Número de neuronas en la 1ª capa (16-256)\n",
    "2. **`hidden_size2`:** Número de neuronas en la 2ª capa (8-128)\n",
    "3. **`learning_rate`:** Opciones discretas [0.001, 0.005, 0.01]\n",
    "4. **`dropout_rate`:** Tasa de dropout (0.0-0.5)\n",
    "5. **`batch_size`:** Tamaño de batch [32, 64, 128, 256]\n",
    "\n",
    "### Tipos de parámetros en Optuna\n",
    "\n",
    "- **`trial.suggest_int()`:** Parámetros enteros (ej: número de neuronas)\n",
    "- **`trial.suggest_float()`:** Parámetros continuos (ej: learning rate)\n",
    "- **`trial.suggest_categorical()`:** Opciones discretas (ej: batch size)\n",
    "\n",
    "**Referencia:** [`Optuna API - Trial object`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo para Optuna.\n",
    "    Optuna intentará MINIMIZAR el valor retornado (validation loss).\n",
    "    \"\"\"\n",
    "    EPOCHS = 10\n",
    "\n",
    "    # Definir el espacio de búsqueda\n",
    "    hidden_layers = ...\n",
    "    hidden_units = ...\n",
    "    dropout_rate = ...\n",
    "    learning_rate = ...\n",
    "    batch_size = ...\n",
    "    \n",
    "    # Crear DataLoader con el batch size correspondiente\n",
    "    train_loader = ...\n",
    "    val_loader = ...\n",
    "\n",
    "    # Definir modelo\n",
    "    model = FlexibleModel(input_size=input_size, \n",
    "                          device=device,\n",
    "                          hidden_units=hidden_units, \n",
    "                          hidden_layers=hidden_layers,\n",
    "                          dropout_rate=dropout_rate)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    history = train_model(model, train_loader, val_loader, EPOCHS, learning_rate)\n",
    "    \n",
    "    return history[\"val_loss\"][-1] # Devolver el MSE en validación al final del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314348ea",
   "metadata": {},
   "source": [
    "## Paso 2: Crear y ejecutar el *Study* de Optuna\n",
    "\n",
    "El objetivo de un *study* es encontrar los mejores hiperparámetros a partir de múltiples pruebas o *trials*.\n",
    "\n",
    "**Opciones principales:**\n",
    "- `direction='minimize'` o `'maximize'` - ¿qué optimizamos?\n",
    "- `sampler` - Algoritmo de búsqueda (TPE por defecto - muy bueno)\n",
    "- `pruner` - Detiene trials malos temprano\n",
    "\n",
    "**Referencia:** [`optuna.create_study()`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62025dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 10 # Número de configuraciones a probar\n",
    "\n",
    "# Crear study\n",
    "study = ...\n",
    "\n",
    "# Ejecutar study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10833694",
   "metadata": {},
   "source": [
    "## Paso 3: Analizar resultados de Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"\\n MEJOR CONFIGURACIÓN ENCONTRADA\\n\")\n",
    "print(f\"Validation Loss: {best_trial.value:.6f}\")\n",
    "print(f\"\\nHyperparámetros:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75704a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de trials\n",
    "trials_df = study.trials_dataframe()\n",
    "print(f\"\\nResumen de {len(trials_df)} trials realizados:\")\n",
    "print(trials_df[['number', 'value', 'state']].head(10))\n",
    "\n",
    "# Estadísticas\n",
    "print(f\"\\nEstadísticas:\")\n",
    "print(f\"  Mejor valor (min): {trials_df['value'].min():.6f}\")\n",
    "print(f\"  Peor valor (max): {trials_df['value'].max():.6f}\")\n",
    "print(f\"  Promedio: {trials_df['value'].mean():.6f}\")\n",
    "print(f\"  Trials completados: {len(trials_df[trials_df['state'] == 'COMPLETE'])}\")\n",
    "print(f\"  Trials prunados: {len(trials_df[trials_df['state'] == 'PRUNED'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f994e",
   "metadata": {},
   "source": [
    "## Paso 4: Entrenar modelo final con los mejores hyperparámetros\n",
    "‼️ Una vez se ha encontrado el valor óptimo de los hiperparámetros, se entrena el modelo final utilizando como datos de entrenamiento la combinación de los sets de entrenamiento y validación y evaluando los resultados en el set de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed99070",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# Extraer los mejores hyperparámetros\n",
    "best_params = ...\n",
    "\n",
    "# Combinar train set + validation set\n",
    "X_train_final = torch.FloatTensor(np.concat([X_train, X_val], axis=0)).to(device)\n",
    "y_train_final = torch.FloatTensor(np.concat([y_train, y_val], axis=0)).to(device).reshape(-1,1)\n",
    "train_ds = TensorDataset(X_train_final, y_train_final)\n",
    "\n",
    "# Crear DataLoader con el batch size correspondiente\n",
    "train_loader = DataLoader(train_ds, batch_size=best_params[\"batch_size\"])\n",
    "test_loader = DataLoader(test_ds, batch_size=best_params[\"batch_size\"])\n",
    "\n",
    "# Definir modelo\n",
    "final_model = FlexibleModel(\n",
    "                            input_size = input_size,\n",
    "                            device = device,\n",
    "                            hidden_layers=best_params['hidden_layers'],\n",
    "                            hidden_units=best_params['hidden_units'],\n",
    "                            dropout_rate=best_params['dropout_rate']\n",
    "                        ).to(device)\n",
    "\n",
    "# Entrenar modelo\n",
    "history = train_model(final_model, train_loader, test_loader, EPOCHS, lr = best_params[\"learning_rate\"])\n",
    "\n",
    "plot_learning_curve(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e61e55e",
   "metadata": {},
   "source": [
    "## Paso 5: Evaluación final en set de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_vs_obs(model, X_obs, y_obs, ax=None, title=None, label=None):\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        device = model.device\n",
    "        X_obs = torch.FloatTensor(X_obs).to(device)\n",
    "        y_pred = model(X_obs).detach().numpy().squeeze()\n",
    "    else:\n",
    "        y_pred = model.predict(X_obs)\n",
    "    ax.scatter(y_obs, y_pred, alpha=0.5, label=label, c=\"blue\", s=1.0)\n",
    "    ax.set_xlabel(\"Observations\")\n",
    "    ax.set_ylabel(\"Predictions\")\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    if len(y_obs.shape)>1:\n",
    "        y_obs = y_obs.squeeze()\n",
    "    if len(y_pred.shape)>1:\n",
    "        y_pred = y_pred.squeeze()\n",
    "    p1 = max(max(y_obs), max(y_pred))\n",
    "    p2 = min(min(y_obs), min(y_pred))\n",
    "    ax.plot([p2, p1], [p2, p1], \"--k\")\n",
    "    ax.legend()\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "fig, axes = plt.subplots(1,2,sharey=True )\n",
    "\n",
    "plot_pred_vs_obs(final_model, X_train_final, y_train_final, axes[0], label = \"Train+Val\")\n",
    "plot_pred_vs_obs(final_model, X_test.values, y_test, axes[1], label = \"Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
